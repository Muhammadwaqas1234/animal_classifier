# -*- coding: utf-8 -*-
"""animal_classifier_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12uKsW3doDipSJNkBYqgMcnEa7EoHbnZ0
"""

from google.colab import files
files.upload()

!pip install kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d alessiocorrado99/animals10
!unzip animals10.zip

!ls -F raw-img/

import tensorflow as tf

DATA_DIR = "/content/raw-img/"
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 42

dataset = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="categorical",
    image_size=IMAGE_SIZE,
    interpolation="nearest",
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=SEED,
)

print("Class Names:", dataset.class_names)

import os
import random
import matplotlib.pyplot as plt
from PIL import Image

DATA_DIR = "/content/raw-img/"
NUM_SAMPLES_PER_CLASS = 2

try:
    class_names = sorted(os.listdir(DATA_DIR))
except FileNotFoundError:
    print(f"Error: Directory not found at {DATA_DIR}.")
    class_names = []

if len(class_names) < 10:
    print(f"Warning: Only {len(class_names)} classes detected.")

if class_names:
    fig, axes = plt.subplots(len(class_names), NUM_SAMPLES_PER_CLASS, figsize=(10, 4 * len(class_names)))
    plt.subplots_adjust(hspace=0.5)

    for i, class_name in enumerate(class_names):
        class_path = os.path.join(DATA_DIR, class_name)
        image_files = [
            f for f in os.listdir(class_path)
            if os.path.isfile(os.path.join(class_path, f)) and f.lower().endswith((".jpg", ".jpeg", ".png"))
        ]

        images_to_display = (
            random.sample(image_files, NUM_SAMPLES_PER_CLASS)
            if len(image_files) >= NUM_SAMPLES_PER_CLASS
            else image_files
        )

        if len(image_files) < NUM_SAMPLES_PER_CLASS:
            print(f"Warning: Class '{class_name}' contains only {len(image_files)} images.")

        for j, image_file in enumerate(images_to_display):
            img = Image.open(os.path.join(class_path, image_file))
            ax = axes[i, j]
            ax.imshow(img)
            ax.axis("off")
            if j == 0:
                ax.set_title(f"Class: {class_name.upper()}", fontsize=12, fontweight="bold")

    plt.suptitle("Sample Images from Animals10 Dataset", y=0.995, fontsize=16)
    plt.show()

# Cell 1: Imports and Setup
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Rescaling
from tensorflow.keras.applications import MobileNetV2
import numpy as np
import os
import pickle

# --- Configuration Constants ---
# Assuming your data is in this directory. Adjust as needed.
DATA_DIR = "/content/raw-img/"
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 42

# Splits (Note: Total images is an estimation, Keras will calculate actual counts)
VAL_SPLIT = 0.1
TEST_SPLIT = 0.1

print("TensorFlow Version:", tf.__version__)
print("Setup complete.")

# Cell 2: Data Loading and Splitting (MODIFIED: Cache/Prefetch removed)

# 1. Load Training Data (80% of total)
train_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="categorical",
    image_size=IMAGE_SIZE,
    interpolation="nearest",
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=SEED,
    subset="training",
    validation_split=VAL_SPLIT + TEST_SPLIT,
)

# Extract Class Names (Essential for deployment)
CLASS_NAMES = train_ds.class_names
NUM_CLASSES = len(CLASS_NAMES)
print(f"\nClass Names: {CLASS_NAMES}")
print(f"Number of classes: {NUM_CLASSES}")

# 2. Load Validation/Test Data (The remaining 20%)
val_test_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="categorical",
    image_size=IMAGE_SIZE,
    interpolation="nearest",
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=SEED,
    subset="validation",
    validation_split=VAL_SPLIT + TEST_SPLIT,
)

# 3. Split the 20% into Validation (10%) and Test (10%)
val_test_batches = tf.data.experimental.cardinality(val_test_ds).numpy()
test_ds = val_test_ds.take(val_test_batches // 2)
val_ds = val_test_ds.skip(val_test_batches // 2)

print("\nDatasets are loaded without caching/prefetching.")
print("Training Set Batches:", tf.data.experimental.cardinality(train_ds).numpy())
print("Validation Set Batches:", tf.data.experimental.cardinality(val_ds).numpy())
print("Testing Set Batches:", tf.data.experimental.cardinality(test_ds).numpy())

# Cell 3: Model Definition

base_model = MobileNetV2(
    input_shape=IMAGE_SIZE + (3,),
    include_top=False,
    weights="imagenet"
)

# Initially freeze the base model for feature extraction
base_model.trainable = False

# Define the complete model with an internal Rescaling layer
model = Sequential([
    # 1. Normalization Layer: Scales [0, 255] to [0, 1]
    Rescaling(1.0 / 255),

    # 2. Base Model (Feature Extractor)
    base_model,

    # 3. Classification Head
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation="softmax")
])

print("MobileNetV2 Model Structure with internal Rescaling:")
model.summary()

# Cell 4: Training, Evaluation, and Saving (Modified to use only simple prefetch during fit)

# Hyperparameters
INITIAL_EPOCHS = 2       # Phase 1: Feature Extraction
TOTAL_EPOCHS = 3        # Phase 2: Total epochs
UNFREEZE_LAYERS = 3      # Number of top layers to unfreeze
initial_lr = 1e-3
fine_tune_lr = 1e-5
AUTOTUNE = tf.data.AUTOTUNE # Need this import here for the optimization

# Apply prefetch to datasets for fitting
train_ds_perf = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds_perf = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds_perf = test_ds.prefetch(buffer_size=AUTOTUNE) # For evaluation

# --- PHASE 1: Feature Extraction ---
print(f"\n--- 4. PHASE 1: Feature Extraction (Epochs 1-{INITIAL_EPOCHS}) ---")
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
history_transfer = model.fit(
    train_ds_perf,
    epochs=INITIAL_EPOCHS,
    validation_data=val_ds_perf
)

# --- PHASE 2: Fine-Tuning ---
print(f"\n--- 5. PHASE 2: Fine-Tuning (Epochs {INITIAL_EPOCHS + 1}-{TOTAL_EPOCHS}) ---")
base_model.trainable = True
for layer in base_model.layers[:-UNFREEZE_LAYERS]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

history_fine_tune = model.fit(
    train_ds_perf,
    epochs=TOTAL_EPOCHS,
    initial_epoch=history_transfer.epoch[-1] + 1,
    validation_data=val_ds_perf
)

# --- MODEL EVALUATION ---
loss, accuracy = model.evaluate(test_ds_perf)
print(f"\n✅ Test Loss: {loss:.4f}")
print(f"✅ Test Accuracy: {accuracy:.4f}")

# --- SAVING FOR DEPLOYMENT ---
MODEL_PATH = "image_classifier_model.h5"
CLASS_NAMES_PATH = "class_names.pkl"

# Save the model
model.save(MODEL_PATH)
print(f"\nModel saved successfully to: {MODEL_PATH}")

# Save the class names list
with open(CLASS_NAMES_PATH, 'wb') as f:
    pickle.dump(CLASS_NAMES, f)
print(f"Class names saved successfully to: {CLASS_NAMES_PATH}")

# Final Cell: Code to Download Model and Class Names

from google.colab import files

MODEL_PATH = "image_classifier_model.h5"
CLASS_NAMES_PATH = "class_names.pkl"

print("Starting download of saved files...")

# Download the trained Keras model
files.download(MODEL_PATH)

# Download the class names pickle file
files.download(CLASS_NAMES_PATH)

print("\nFiles have been successfully downloaded to your local machine (or browser's default download folder).")
print(f"You should have: 1. {MODEL_PATH} and 2. {CLASS_NAMES_PATH}")

# --- REVISED SAVING FOR DEPLOYMENT ---
MODEL_PATH = "image_classifier_model.keras" # Change extension here!
CLASS_NAMES_PATH = "class_names.pkl"

# Save the model using the native Keras format
model.save(MODEL_PATH)
print(f"\nModel saved successfully to: {MODEL_PATH}")
files.download(MODEL_PATH)

# ... (rest of the saving code)

# --- REVISED SAVING FOR DEPLOYMENT: SAVE WEIGHTS ONLY ---
MODEL_WEIGHTS_PATH = "image_classifier_weights.weights.h5" # Save weights with .h5 or .keras extension
CLASS_NAMES_PATH = "class_names.pkl"

# 1. Save only the weights
model.save_weights(MODEL_WEIGHTS_PATH)
print(f"\nModel weights saved successfully to: {MODEL_WEIGHTS_PATH}")

# 2. Save the class names list
import pickle
with open(CLASS_NAMES_PATH, 'wb') as f:
    pickle.dump(CLASS_NAMES, f)
print(f"Class names saved successfully to: {CLASS_NAMES_PATH}")

# 3. Download the files
from google.colab import files
files.download(MODEL_WEIGHTS_PATH)
files.download(CLASS_NAMES_PATH)

import matplotlib.pyplot as plt

# Metrics from Transfer Learning phase
acc = history_transfer.history["accuracy"]
val_acc = history_transfer.history["val_accuracy"]
loss = history_transfer.history["loss"]
val_loss = history_transfer.history["val_loss"]

# Append metrics from Fine-Tuning phase
acc += history_fine_tune.history["accuracy"]
val_acc += history_fine_tune.history["val_accuracy"]
loss += history_fine_tune.history["loss"]
val_loss += history_fine_tune.history["val_loss"]

# Epoch numbers for plotting
epochs = range(len(acc))

print(f"\nTotal Epochs Visualized: {len(epochs)}")

plt.figure(figsize=(12, 4))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label="Training Accuracy")
plt.plot(epochs, val_acc, label="Validation Accuracy")
plt.axvline(x=FINE_TUNE_AT - 1, color="r", linestyle="--", label="Fine-Tuning Start")
plt.title("Training and Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc="lower right")
plt.grid(True)

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label="Training Loss")
plt.plot(epochs, val_loss, label="Validation Loss")
plt.axvline(x=FINE_TUNE_AT - 1, color="r", linestyle="--", label="Fine-Tuning Start")
plt.title("Training and Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(loc="upper right")
plt.grid(True)

plt.show()